{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3edb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HUB_CACHE=/ist-nas/ist-share/vision/huggingface_hub/\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env HF_HUB_CACHE=/ist-nas/ist-share/vision/huggingface_hub/\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa4997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda, dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import yaml\n",
    "import psutil\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import AutoencoderKLWan, WanTransformer3DModel\n",
    "from diffusers.utils import load_video, export_to_video\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from transformers import UMT5EncoderModel, AutoTokenizer\n",
    "from accelerate import cpu_offload\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    torch.backends.cudnn.allow_tf32 = False\n",
    "seed_everything(0)\n",
    "\n",
    "MODEL_ID = \"Wan-AI/Wan2.1-T2V-14B-Diffusers\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "vae, scheduler, tokenizer, text_encoder, transformer = None, None, None, None, None\n",
    "video_processor, mask_processor = None, None\n",
    "\n",
    "print(f\"using device: {DEVICE}, dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412f8cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2230183bf29d48a099cb535ffd5ef2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6805c3437b994dcfb2e60d88bfdb1970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae scale factor temporal: 4, spatial: 8\n"
     ]
    }
   ],
   "source": [
    "if vae is None:\n",
    "    vae = AutoencoderKLWan.from_pretrained(MODEL_ID, subfolder=\"vae\", torch_dtype=DTYPE, local_files_only=True)\n",
    "    # vae.enable_tiling()\n",
    "    vae_scale_factor_temporal = 2 ** sum(vae.temperal_downsample)\n",
    "    vae_scale_factor_spatial = 2 ** len(vae.temperal_downsample)\n",
    "if scheduler is None:\n",
    "    scheduler = UniPCMultistepScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\", local_files_only=True)\n",
    "if tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\", model_max_length=512, local_files_only=True)\n",
    "if text_encoder is None:\n",
    "    text_encoder = UMT5EncoderModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\", torch_dtype=DTYPE, local_files_only=True)\n",
    "if transformer is None:\n",
    "    transformer = WanTransformer3DModel.from_pretrained(MODEL_ID, subfolder=\"transformer\", torch_dtype=DTYPE, local_files_only=True)\n",
    "    transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=DTYPE)\n",
    "\n",
    "models = [text_encoder, transformer, vae]\n",
    "for m in models:\n",
    "    cpu_offload(m, DEVICE) # automatically move unused models to cpu\n",
    "\n",
    "video_processor = VideoProcessor(vae_scale_factor=vae_scale_factor_spatial)\n",
    "mask_processor = VideoProcessor(vae_scale_factor=vae_scale_factor_spatial, do_binarize=True, do_normalize=False, do_convert_grayscale=True)\n",
    "\n",
    "print(f'vae scale factor temporal: {vae_scale_factor_temporal}, spatial: {vae_scale_factor_spatial}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da05b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def track_memory_usage():\n",
    "    peak_memory = {}\n",
    "    torch.cuda.reset_peak_memory_stats(DEVICE)\n",
    "    start_gpu = torch.cuda.memory_allocated(DEVICE)/(1024**3)\n",
    "    start_cpu = psutil.Process(os.getpid()).memory_info().rss/(1024**3)\n",
    "\n",
    "    try:\n",
    "        yield peak_memory\n",
    "    finally:\n",
    "        peak_memory[\"gpu_gb\"] = round(torch.cuda.max_memory_allocated(DEVICE)/(1024**3), 2)\n",
    "        current_gpu = torch.cuda.memory_allocated(DEVICE)/(1024**3)\n",
    "        peak_memory[\"cpu_gb\"] = round(psutil.Process(os.getpid()).memory_info().rss/(1024**3), 2)\n",
    "        print(f\"gpu peak: {peak_memory['gpu_gb']} gb, current: {current_gpu:.2f} gb, start: {start_gpu:.2f} gb\")\n",
    "        print(f'cpu rss: {peak_memory[\"cpu_gb\"]} gb, start: {start_cpu:.2f} gb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c45827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sdedit_video_inpainting_pipeline(\n",
    "    input_video_frames,   # List[PIL.Image.Image]\n",
    "    input_mask_frames,    # b f c h w\n",
    "    height,\n",
    "    width,\n",
    "    prompt,\n",
    "    neg_prompt,\n",
    "    strength,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    dir,\n",
    "    debug,\n",
    "):\n",
    "    print('1. preparing timesteps...')\n",
    "    scheduler.set_timesteps(num_inference_steps, DEVICE)\n",
    "    original_timesteps = scheduler.timesteps\n",
    "\n",
    "    init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "    start_step = max(num_inference_steps - init_timestep, 0) \n",
    "    timesteps = original_timesteps[start_step * scheduler.order :]\n",
    "    num_inference_steps = num_inference_steps - start_step\n",
    "    latent_timestep = timesteps[:1] # shape: [1]\n",
    "    print(f'scheduler config: {scheduler.config}')\n",
    "    print(f'active timesteps (strength={strength}, {len(timesteps)}/{len(original_timesteps)} steps): {timesteps.cpu().numpy()}')\n",
    "\n",
    "    print(f'\\n2. encoding video frames...')\n",
    "    video_tensor = video_processor.preprocess_video(input_video_frames, height, width).to(DEVICE, DTYPE) # [B, C, F, H, W], range [0, 1]\n",
    "\n",
    "    num_channels_latents = transformer.config.in_channels # 16\n",
    "    shape = (\n",
    "        1,                                                              # b\n",
    "        num_channels_latents,                                           # latent c\n",
    "        (video_tensor.size(2) - 1) // vae_scale_factor_temporal + 1,    # latent f\n",
    "        height // vae_scale_factor_spatial,                             # latent h\n",
    "        width // vae_scale_factor_spatial,                              # latent w\n",
    "    )\n",
    "    initial_latents = [vae.encode(vid.unsqueeze(0)).latent_dist.sample() for vid in video_tensor]\n",
    "    initial_latents = torch.cat(initial_latents, dim=0).to(DTYPE)\n",
    "\n",
    "    latents_mean = (torch.tensor(vae.config.latents_mean).view(1, vae.config.z_dim, 1, 1, 1).to(DEVICE, DTYPE))\n",
    "    latents_std = 1.0 / torch.tensor(vae.config.latents_std).view(1, vae.config.z_dim, 1, 1, 1).to(DEVICE, DTYPE)\n",
    "    init_latents = (initial_latents - latents_mean) * latents_std \n",
    "\n",
    "    noise = torch.randn(shape).to(DEVICE)\n",
    "    latents = scheduler.add_noise(init_latents, noise, latent_timestep)\n",
    "    print('latents', latents.shape)\n",
    "\n",
    "    print(f'\\n3. preparing masks...')\n",
    "    mask_tensor = mask_processor.preprocess_video(input_mask_frames, height, width).to(DEVICE, DTYPE) # [B, 1, F, H, W], (0, 1)\n",
    "    \n",
    "    mask = torch.nn.functional.interpolate(\n",
    "        mask_tensor, \n",
    "        size=shape[2:],\n",
    "        mode='trilinear'\n",
    "    ).to(DEVICE, DTYPE)\n",
    "    print('mask', mask.shape)\n",
    "\n",
    "    print(f'\\n4. encoding prompts...')\n",
    "    pos_inputs = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    pos_input_ids = pos_inputs.input_ids\n",
    "    pos_attention_mask = pos_inputs.attention_mask # [1, 512]\n",
    "    pos_seq_lens = pos_attention_mask.gt(0).sum(dim=1).long()\n",
    "    pos_text_embeds = text_encoder(pos_input_ids.to(DEVICE), pos_attention_mask.to(DEVICE)).last_hidden_state \n",
    "    pos_text_embeds = pos_text_embeds.to(DEVICE, DTYPE) # [1, 512, 4096]\n",
    "    pos_trimmed_embedds = [u[:v] for u, v in zip(pos_text_embeds, pos_seq_lens)] # [1, seq_len, 4096]\n",
    "    prompt_embeds = torch.stack([\n",
    "        torch.cat([u, u.new_zeros(tokenizer.model_max_length - u.size(0), u.size(1))]) for u in pos_trimmed_embedds\n",
    "    ], dim=0) # [1, 512, 4096]\n",
    "\n",
    "    neg_inputs = tokenizer(neg_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    neg_input_ids = neg_inputs.input_ids\n",
    "    neg_attention_mask = neg_inputs.attention_mask\n",
    "    neg_seq_lens = neg_attention_mask.gt(0).sum(dim=1).long()\n",
    "    neg_text_embeds = text_encoder(neg_input_ids.to(DEVICE), neg_attention_mask.to(DEVICE)).last_hidden_state\n",
    "    neg_text_embeds = neg_text_embeds.to(DEVICE, DTYPE)\n",
    "    neg_trimmed_embeds = [u[:v] for u, v in zip(neg_text_embeds, neg_seq_lens)]\n",
    "    negative_prompt_embeds = torch.stack([torch.cat([\n",
    "        u, u.new_zeros(tokenizer.model_max_length - u.size(0), u.size(1))]) \n",
    "        for u in neg_trimmed_embeds\n",
    "    ], dim=0)\n",
    "\n",
    "    prompt_embeds.to(DTYPE)\n",
    "    negative_prompt_embeds.to(DTYPE)\n",
    "    print('prompt', prompt_embeds.shape)\n",
    "    print('negative prompt', negative_prompt_embeds.shape)\n",
    "\n",
    "    print(f'\\n5. denoising...')\n",
    "    if debug: torch.save(latents, f'{dir}/latents_before_denoising.pt')\n",
    "    progress_bar = tqdm(timesteps, total=num_inference_steps)\n",
    "    for i, t in enumerate(progress_bar):\n",
    "        latent_model_input = latents.to(DTYPE)\n",
    "\n",
    "        noise_pred = transformer(\n",
    "            hidden_states=latent_model_input,\n",
    "            timestep=t.expand(1), # convert scalar to [t]\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        noise_pred_uncond = transformer(\n",
    "            hidden_states=latent_model_input,\n",
    "            timestep=t.expand(1),\n",
    "            encoder_hidden_states=negative_prompt_embeds,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred - noise_pred_uncond)\n",
    "\n",
    "        # x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "        # sdedit\n",
    "        init_latents_proper = init_latents\n",
    "        init_mask = mask\n",
    "        if i < len(timesteps) - 1:\n",
    "            noise_timestep = timesteps[i+1]\n",
    "            init_latents_proper = scheduler.add_noise(init_latents_proper, noise, torch.tensor([noise_timestep]))\n",
    "        latents = (1-init_mask) * init_latents_proper + init_mask * latents\n",
    "    if debug: torch.save(latents, f'{dir}/latents_after_denoising.pt')\n",
    "\n",
    "    print(f'\\n6. decoding...')\n",
    "    latents = latents.to(vae.dtype)\n",
    "    latents_mean = (torch.tensor(vae.config.latents_mean).view(1, vae.config.z_dim, 1, 1, 1).to(latents.device, latents.dtype))\n",
    "    latents_std = 1.0 / torch.tensor(vae.config.latents_std).view(1, vae.config.z_dim, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "    latents = latents/latents_std + latents_mean\n",
    "\n",
    "    if debug: torch.save(latents, f'{dir}/latents_before_decode.pt')\n",
    "    output = vae.decode(latents, return_dict=False)[0] # [1, 3, F, H, W]\n",
    "    output = video_processor.postprocess_video(output) # (F, 3, H, W)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb6dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. preparing timesteps...\n",
      "scheduler config: FrozenDict([('num_train_timesteps', 1000), ('beta_start', 0.0001), ('beta_end', 0.02), ('beta_schedule', 'linear'), ('trained_betas', None), ('solver_order', 2), ('prediction_type', 'flow_prediction'), ('thresholding', False), ('dynamic_thresholding_ratio', 0.995), ('sample_max_value', 1.0), ('predict_x0', True), ('solver_type', 'bh2'), ('lower_order_final', True), ('disable_corrector', []), ('solver_p', None), ('use_karras_sigmas', False), ('use_exponential_sigmas', False), ('use_beta_sigmas', False), ('use_flow_sigmas', True), ('flow_shift', 3.0), ('timestep_spacing', 'linspace'), ('steps_offset', 0), ('final_sigmas_type', 'zero'), ('rescale_betas_zero_snr', False), ('_class_name', 'UniPCMultistepScheduler'), ('_diffusers_version', '0.33.0.dev0')])\n",
      "active timesteps (strength=0.7, 7/10 steps): [874 817 749 666 562 428 249]\n",
      "\n",
      "2. encoding video frames...\n",
      "latents torch.Size([1, 16, 11, 160, 90])\n",
      "\n",
      "3. preparing masks...\n",
      "mask torch.Size([1, 1, 11, 160, 90])\n",
      "\n",
      "4. encoding prompts...\n",
      "prompt torch.Size([1, 512, 4096])\n",
      "negative prompt torch.Size([1, 512, 4096])\n",
      "\n",
      "5. denoising...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066583ee1cfa4a30b026ad940a73f157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. decoding...\n",
      "gpu peak: 10.29 gb, current: 0.06 gb, start: 0.00 gb\n",
      "cpu rss: 39.49 gb, start: 38.08 gb\n",
      "\n",
      "exporting to video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/sdedit/0519/1236/out_nb.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dir: str = f'output/sdedit/{time.strftime(\"%m%d\")}/{time.strftime(\"%H%M\")}'\n",
    "    input_video: str = \"input/landmark/process/16fps_720x1280_centercrop_41/man_video.mp4\"\n",
    "    height: int = 1280\n",
    "    width: int = 720\n",
    "    strength: float = 0.7\n",
    "    num_inference_steps: int = 10\n",
    "    guidance_scale: float = 7.0\n",
    "    # prompt: str = \"A man is speaking straight to the camera. He is bald, has beard, and is wearing a white shirt. His mouth opens and closes, naturally revealing his teeth as he gives his speech. He is eloquently pronouncing each word, moving his head and changing his facial expression as he talks.\"\n",
    "    prompt: str = \"A man with beautiful teeth speaking\"\n",
    "    # prompt: str = \"A professional man speaking with a flawless, radiant smile—symmetrical white teeth, no gaps or imperfections, and a natural-looking dental appearance.\"\n",
    "    neg_prompt: str = \"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\"\n",
    "    fps: int = 16\n",
    "\n",
    "    timing_stats: Dict[str, float] = field(default_factory=dict)\n",
    "    memory_stats: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def pipeline_kwargs(self):\n",
    "        return {k: v for k, v in asdict(self).items() if k in ['dir', 'height', 'width', 'strength', 'num_inference_steps', 'guidance_scale', 'prompt', 'neg_prompt']}\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.dir, exist_ok=True)\n",
    "config_file = f'{config.dir}/config.yml'\n",
    "with open(config_file, \"w\") as f:\n",
    "    yaml.safe_dump(asdict(config), f, sort_keys=False)\n",
    "\n",
    "start_inference = time.time()\n",
    "with track_memory_usage() as peak_memory:\n",
    "    inpainted_video = sdedit_video_inpainting_pipeline(\n",
    "        input_video_frames=load_video(config.input_video),\n",
    "        input_mask_frames=load_video(config.input_video.replace('video', 'mask')),\n",
    "        debug=False,\n",
    "        **config.pipeline_kwargs,\n",
    "    )[0]\n",
    "infer_time = time.time() - start_inference\n",
    "\n",
    "config.timing_stats.update({\"infer_seconds\": round(infer_time, 2)})\n",
    "config.memory_stats.update(peak_memory)\n",
    "with open(config_file, \"w\") as f:\n",
    "    yaml.safe_dump(asdict(config), f, sort_keys=False)\n",
    "\n",
    "print(f'\\nexporting to video...')\n",
    "export_to_video(inpainted_video, f'{config.dir}/out_nb.mp4', fps=config.fps, quality=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1024d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 6, 240, 136])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents = torch.load('output/sdedit/0514/2157/latents_before_decode.pt')\n",
    "latents.shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wan-inpaint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
